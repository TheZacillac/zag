# RAG System Environment Configuration
# Copy this file to .env and adjust values for your setup

# ==============================================
# Database Configuration
# ==============================================
POSTGRES_DB=ragdb
POSTGRES_USER=rag
POSTGRES_PASSWORD=ragpass

# Full database URL (used by Python services)
DATABASE_URL=postgresql://rag:ragpass@db:5432/ragdb

# ==============================================
# Ollama Configuration
# ==============================================

# IMPORTANT: Choose the right OLLAMA_HOST for your setup:

# Option 1: Ollama running on the host machine (Docker Desktop on Mac/Windows)
# Use this if you're running Ollama on your laptop/desktop alongside Docker
OLLAMA_HOST=http://host.docker.internal:11434

# Option 2: Ollama running on a specific server/IP
# Use this if Ollama is on a different machine in your network
# OLLAMA_HOST=http://192.168.7.215:11434

# Option 3: Ollama as a Docker service (if you add it to docker-compose)
# OLLAMA_HOST=http://ollama:11434

# ==============================================
# Model Configuration
# ==============================================

# Embedding model (must be available in your Ollama instance)
# Popular options:
# - embeddinggemma (default, good general purpose)
# - nomic-embed-text
# - all-minilm
EMBED_MODEL=embeddinggemma

# Reranking model (for improving search relevance)
# This can be different from the embedding model
RERANK_MODEL=all-minilm

# Chat/Generation model (for the TUI)
# Popular options:
# - llama3.2 (fast, efficient)
# - mixtral (high quality)
# - gpt-oss:120b (very large, high quality)
CHAT_MODEL=llama3.2

# ==============================================
# Worker Configuration
# ==============================================

# Embedder settings
EMBED_BATCH=64          # Chunks to process per batch
EMBED_SLEEP=15          # Seconds to sleep when no work available
EMBED_TIMEOUT=300       # HTTP timeout in seconds (5 minutes)

# Reranker settings
RERANK_BATCH_SIZE=50    # Chunks to rerank per batch
RERANK_SLEEP_SEC=30     # Seconds to sleep when no work available
RERANK_TIMEOUT=300      # HTTP timeout in seconds

# ==============================================
# Digester Configuration
# ==============================================
DIGEST_PATH=/digestion  # Directory to watch for new files
POLARS_API=http://polars-worker:8080  # Polars worker API endpoint
MAX_RETRIES=3           # Upload retry attempts
RETRY_DELAY=30          # Seconds between retries

# ==============================================
# API Configuration
# ==============================================

# Port mappings (these match docker-compose.yml)
# Change these if you need different ports on your host
POSTGRES_PORT=5432
POLARS_PORT=8080

# ==============================================
# Performance Tuning (Advanced)
# ==============================================

# PostgreSQL connection pool settings
DB_POOL_MIN_SIZE=1
DB_POOL_MAX_SIZE=4

# Vector search settings (adjust based on your data size)
# IVFFLAT lists parameter: use ~sqrt(number_of_rows) as a guideline
# For 10,000 chunks: lists=100 (current default)
# For 100,000 chunks: lists=316
# For 1,000,000 chunks: lists=1000
VECTOR_INDEX_LISTS=100

# ==============================================
# Deployment Notes
# ==============================================

# DEVELOPMENT:
# - Use OLLAMA_HOST=http://host.docker.internal:11434
# - Keep default credentials (they're isolated in Docker)
# - Use ./start.sh for quick startup

# PRODUCTION:
# - Change POSTGRES_PASSWORD to a strong password
# - Use Docker secrets instead of environment variables
# - Add TLS/SSL termination (nginx/traefik)
# - Configure backup strategy for PostgreSQL volume
# - Add authentication to API endpoints
# - Set up monitoring (Prometheus + Grafana)

# ==============================================
# Troubleshooting
# ==============================================

# If embeddings aren't being generated:
# 1. Check that OLLAMA_HOST is reachable from Docker containers
# 2. Verify EMBED_MODEL exists: curl $OLLAMA_HOST/api/tags
# 3. Check embedder logs: docker compose logs -f embedder
# 4. Test Ollama manually: curl $OLLAMA_HOST/api/embed -d '{"model":"embeddinggemma","input":["test"]}'

# If search returns no results:
# 1. Check that embeddings exist in database
# 2. Verify vector dimension matches your model (768 for embeddinggemma)
# 3. Check polars-worker logs: docker compose logs -f polars-worker

# If digester isn't processing files:
# 1. Check that digestion/ directory exists and is mounted
# 2. Verify polars-worker is healthy: curl localhost:8080/healthz
# 3. Check digester logs: docker compose logs -f digester
# 4. Ensure files are being detected (file system events)

