# RAG Chat TUI

Interactive terminal user interface for querying your RAG system with natural language.

## Features

- üé® Beautiful terminal UI with rich formatting
- üí¨ Real-time chat with your documents
- üîç Automatic context retrieval from your RAG database
- ü§ñ Powered by Ollama for local LLM inference
- üìö Shows relevant document chunks used for answers
- üìã Advanced copy/paste support with keyboard shortcuts
- ‚ö° Smart text formatting and clipboard integration

## Usage

### Option 1: Full System with Chat TUI (Recommended)

```bash
# From the project root - Start everything including chat-tui
make up

# OR direct docker compose
docker compose up --build
```

### Option 2: Run with Docker Compose

```bash
# From the project root
docker compose --profile chat up chat-tui

# Or run interactively
docker compose --profile chat run --rm chat-tui
```

### Option 3: Run Standalone

```bash
# Build the container
docker build -t rag-chat-tui .

# Run interactively
docker run -it --rm \
  --network rag_ragnet \
  -e POLARS_API=http://polars-worker:8080 \
  -e OLLAMA_HOST=http://192.168.7.215:11434 \
  -e CHAT_MODEL=llama3.2 \
  rag-chat-tui
```

### Option 4: Run Locally (No Docker)

```bash
# Quick local development
./run-local.sh

# OR manual setup:
# Install dependencies
pip install textual httpx rich pyperclip

# Set environment variables
export POLARS_API=http://localhost:8080
export OLLAMA_HOST=http://192.168.7.215:11434
export CHAT_MODEL=llama3.2

# Run the app
python app.py
```

## Configuration

Environment variables:

- `POLARS_API` - URL of the RAG query service (default: `http://polars-worker:8080`)
- `OLLAMA_HOST` - URL of your Ollama instance (default: `http://192.168.7.215:11434`)
- `CHAT_MODEL` - Ollama model to use (default: `llama3.2`)

## Keyboard Shortcuts

- `Enter` - Send message
- `Ctrl+Y` - Copy last assistant response
- `Ctrl+U` - Copy last user message
- `Ctrl+V` - Paste from clipboard into input
- `Ctrl+A` - Select all text in input field
- `Ctrl+L` - Clear chat history
- `Ctrl+C` - Quit application

## How it Works

1. You type a question
2. The TUI embeds your query using Ollama
3. The RAG system searches for relevant document chunks using vector similarity
4. The top-k most relevant chunks are retrieved
5. The chunks are sent to Ollama as context
6. Ollama generates a response based on the context
7. The response is streamed back to the TUI

## Troubleshooting

**TUI won't start:**
- Make sure polars-worker is running and healthy
- Check that Ollama is accessible at the configured host

**No context found:**
- Ensure documents have been ingested and embedded
- Check that the embedder worker has processed the chunks

**Slow responses:**
- Large models take time to generate responses
- Consider using a smaller/faster model
- Increase `top_k` to get more context (or decrease for faster queries)

**Clipboard issues:**
- Make sure clipboard tools are installed: `xclip`, `xsel`, or `wl-clipboard`
- On Linux: `sudo apt install xclip xsel wl-clipboard`
- On macOS: Clipboard should work out of the box
- On Windows: Clipboard should work out of the box
- If copy/paste fails, check that your terminal supports clipboard operations
